---
title: "Assignments"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

# Assignment 1

### Problem 1 

Install the datasets package on the console below using `install.packages("datasets")`. Now load the library.

```{r}
#Install data set package

#install.packages("datasets")

#Load the library of datasets 

library(datasets)

```

Load the USArrests dataset and rename it `dat`. Note that this dataset comes with R, in the package datasets, so there's no need to load data from your computer. Why is it useful to rename the dataset?

Answer to problem 1: It is useful to rename the dataset because any time you need to analyze the data in any way, you can use the new name to tell the computer to use that data set for the analysis.Also, if you change the data in any way - such as removing outliers - it has to have a new name, so that if someone wants to edit the original data they will not be confused. It is very important for analysis to be replicable, so renaming the data allows you to do what you need to it without taking away someone's else's ability to do their own analysis to the original dataset.

```{r}
#Load USArrests data set

USArrests

#Rename USArrests dat

dat<-USArrests

#Now USArrests data set is called dat
```
### Problem 2

Use this command to make the state names into a new variable called State. 

```{r, eval=TRUE}
dat$state <- tolower(rownames(USArrests))
```

This dataset has the state names as row names, so we just want to make them into a new variable. We also make them all lower case, because that will help us draw a map later - the map function requires the states to be lower case.


List the variables contained in the dataset `USArrests`.

Answer to problem 2: "Murder" "Assault"  "UrbanPop" "Rape" "state"

```{r}
#finding names of state variables

names(dat)
```
### Problem 3 

What type of variable (from the DVB chapter) is `Murder`? 

Answer: Murder is a quantitative variable. 

What R Type of variable is it?

Answer: In terms of R Types of variables Murder is a numeric variable.


### Problem 4

What information is contained in this dataset, in general? What do the numbers mean? 

Answer: The information contained in the dataset has to do with 4 categories of crimes and their frequency of arrest rates per 100,000 residents in all 50 states. The numbers refer to how many instances of arrests for that category of crime occurred in that state per every 100,000 residents. 
  

### Problem 5

Draw a histogram of `Murder` with proper labels and title.

```{r}
#Make a histogram of murder with proper axis titles (Problem 5)

hist(dat$Murder, xlab = "Number of Murder Arrests", main = "Histogram of Murder Arrests")
```

### Problem 6

Please summarize `Murder` quantitatively. What are its mean and median? What is the difference between mean and median? What is a quartile, and why do you think R gives you the 1st Qu. and 3rd Qu.?

Answer to problem 6: The mean of murder is 7.788, the median of murder is 7.250. While mean and median are both measures of central tendency, they have relevant differences. Mean is an average of all of the numbers in the set. It is found by adding up all the numbers in the set and dividing that sum by the total number of variables in the set.The median is the number in the center when you line up all of the variables in numerical order.This means that half of the data lies below the median and the other half above.A quartile represents the three values in a sample that divide the distribution of the data into even fourths.This means that each quartile contains 25% of the data. I think that R gives you the first and third quartiles because these two measures bookend the interquartile range (IQR = Q3-Q1). The interquartile range represents the middle half of the values,and the size of the IQR reveals how spread out the data is. The higher the IQR, the more spread out the data is and vice versa.  


```{r}
#Summarizing Murder quantitatively (Problem 6)

summary(dat$Murder)
```

### Problem 7

Repeat the same steps you followed for `Murder`, for the variables `Assault` and `Rape`. Now plot all three histograms together. You can do this by using the command `par(mfrow=c(3,1))` and then plotting each of the three. 

```{r, echo = TRUE, fig.width = 5, fig.height = 8}
#Make a histogram of Assault with the proper axis titles (Problem 7)

hist(dat$Assault , xlab = "Number of Assaults Arrests" , main = "Histogram of Assault Arrests")

#Make a histogram of Rape with proper axis titles (Problem 7)

hist(dat$Rape , xlab = "Number of Rape Arrests" , main = "Histogram of Rape Arrests")

#Make a plot of all 3 histograms (Murder, Assault, and Rape) together (Problem 7)

par(mfrow=c(3,1)) 
hist(dat$Murder, xlab = "Number of Murder Arrests", main = "Histogram of Murder Arrests")
hist(dat$Assault , xlab = "Number of Assault Arrests" , main = "Histogram of Assault Arrests")
hist(dat$Rape , xlab = "Number of Rape Arrests" , main = "Histogram of Rape Arrests")
```

What does the command par do, in your own words (you can look this up by asking R `?par`)?

Answer: The command par is a way in which multiple plots can be viewed together.It essentially set the parameters for viewing graphs. For example, in this problem I wanted to display 3 graphs each with their own variables.The graphs all have the same y-axis (frequency) and all the x-axes are "number of." This command allows the histograms to be viewed simultaneously and thus the data to be easily analyzed and compared.


What can you learn from plotting the histograms together?

Answer: By plotting the histograms together, you can learn about differences and similarities in the arrest rates for these 3 different categories.
  
### Problem 8

In the console below (not in text), type `install.packages("maps")` and press Enter, and then type `install.packages("ggplot2")` and press Enter. This will install the packages so you can load the libraries.

Run this code:

```{r, eval = FALSE, fig.width = 7.5, fig.height = 4}
library('maps') 
library('ggplot2') 

ggplot(dat, aes(map_id=state, fill=Murder)) + 
  geom_map(map=map_data("state")) + 
  expand_limits(x=map_data("state")$long, y=map_data("state")$lat)
```

What does this code do? Explain what each line is doing.

Answer: The first two lines specify which library to find the right packages in in order to achieve the commands of the code below. The third line specifies Murder as the specific variable for the plot. The fourth line makes a map of the states filled with the state data. The fifth line designates the axes of the map and sets the plot limits, which determines the dimensions of the map.








# Assignment 2

### Problem 1 

```{r}
# Load data

# Read the data

dat <- read.csv(file = "data/Assignment 2 data copy.csv")

# What are the dimensions of the dataset? 

dim(dat)
``` 

### Problem 2 

```{r}
## Variables

# Describe the variables in the dataset.

names(dat)

```

"mjage"= age at which respondent first used marijuana or hashish
"cigage"= age at which respondent first started smoking cigarettes every day 
"iralcage"= age at which respondent first tried alcohol
"age2"= final edited ages since respondents could change their answer in the course of the survey
"sexatract"= sexual attraction 
"speakengl"= how well respondent speaks english
"irsex"= gender (male or female)

Answer:"mjage," "cigage," and "iralcage" are all quantitative numeric variables because they contain numerical values with measurement units and do not simply assign respondents to a group.They can also be evaluated for measures of central tendency such as mean, median, and mode."age2,""sexatract," "speakengl,"and "irsex" are all categorical variables because they assign respondents to groups and cannot be quantitatively evaluated. 


What is this data set about? Who collected the data, what kind of sample is it, and what was the purpose of generating the data?
This data set is about drug use and health in the US and is aptly named, The National Survey on Drug Use and Health. More specifically,it tracked trends in substance use and mental illness. It is a representative sample of the general US civilian population aged 12 and up that was collected by the Substance Abuse and Mental Health Services Administration branch of the US Department of Health and Human Services. The purpose of generating the data was to provide information that would aid in prevention and treatment and estimate its need, monitor trends in substance use/abuse, and inform public policy regarding health.

### Problem 3
```{r}
## Age and gender

# What is the age distribution of the sample like? Make sure you read the code book to know what the variable values mean.
#Make a histogram of age. 

hist(dat$age2 , main = "histogram of final age categories",  xlab= "age category")
```

Answer: The ages are weighted more toward the older age categories. Most of the respondents fell into the older age range of the data,between groups 12 and 16 and thus the ages 24-64. The majority of the data, and thus the ages lies in those two bins.

Do you think this age distribution representative of the US population? Why or why not?

Answer: I think this age distribution is representative of the US population because census data was used in conducting the survey.
All 50 states plus DC were included and surveyors were sent to many houses that were randomly sampled and selected 2 members (12 or older) of the household to respond. This may be where there could be an issue with representation if younger members of the families or households were disproportionately selected.Another issue could be that they did not survey nursing homes or other institutional group quarters, which might skew the age distribution. Overall I think the methodology of sampling was such that this must be a representative sample of the age distribution of the US. 

Is the sample balanced in terms of gender? If not, are there more females or males?

Answer: The sample does seem to be balanced in terms of gender except bins 6 and 7 are comprised of males only and bin 8 only females.Overall there are more males than females in the sample, and in terms of the split among age categories, men make up the majority of most of them.

```{r}
# Use this code to draw a stacked bar plot to view the relationship between sex and age. What can you conclude from this plot?

tab.agesex <- table(dat$irsex, dat$age2)
barplot(tab.agesex,
        main = "Stacked barchart",
        xlab = "Age category", ylab = "Frequency",
        legend.text = rownames(tab.agesex),
        beside = FALSE) # Stacked bars (default)
```

### Problem 4 
```{r}
## Substance use

#Make a boxplot

boxplot(dat$mjage , dat$cigage , dat$iralcage , names=c("Marijuana" , "Cigarettes" , "Alcohol"))
```

For which of the three substances included in the dataset (marijuana, alcohol, and cigarettes) do individuals tend to use the substance earlier?

Answer:Individuals tend to use alcohol the earliest. Based on the boxplot, it has the lowest median age as well as the lowest minimum value.

### Problem 5 
```{r}
## Sexual attraction

# What does the distribution of sexual attraction look like? Is this what you expected?

#Subset the data to remove legitimate skips  

newdata<-subset(dat$sexatract,subset = dat$sexatract<7 )

#Make a histogram of the new sexattract data set 

hist(newdata, xlab = "Sexual Attraction Rated on a Scale of 1-6", main = "Histogram of Sexual Attraction")

```

Answer: The distribution of sexual attraction is skewed to the right, with most of the respondents selecting "1" on the scale, which means that they are only attracted to the opposite sex. The second most frequent response was "2" which means they are mostly attracted to the opposite sex.The lowest number of the respondents said that they were not sure which was response "6." This is mostly expected because the majority of people in the US are heterosexual.

```{r}
#What is the distribution of sexual attraction by gender?

#Make a data frame with sexatract and irsex variables included

dat5<-data.frame(dat$sexatract, dat$irsex)

table(dat5$dat.irsex, dat5$dat.sexatract)


```

Answer:More males than females responded "1" however more females than males gave "2," "3," "4," and "6" as their answers.This suggests that women in this study were more likely to be mostly attracted to the same sex, equally attracted to both,mostly attracted to the same sex. Most men in the study are strictly heterosexual, while the women seem to see more variance.This answer is based on the table I made (non-graphical EDA)

### Problem 6 
```{r}
## English speaking

# What does the distribution of English speaking look like in the sample? Is this what you might expect for a random sample of the US population?

hist(dat$speakengl, main = "How Well Respondents Speak English", xlab = "English Proficiency on a Scale of 1-4 (1 being 'very well' 4 being 'not at all'", xlim = c(1,4))


```

Answer: A vast majority of the respondents report speaking English very well. While only a small proportion reported not speaking it well. No respondents reported not speaking English at all. This was definitely expected from a sample of the US population because, as English is the country's official language, most people here speak it.

```{r}
#Are there more English speaker females or males?

table(dat$speakengl,dat$irsex)


```

Answer:Almost equal numbers of men and women responded that they spoke English very well (response "1"), 84 and 77 respondents respectively.7 times more men than women reported speaking English well(response "2"), while only women and no men admitted that they did not speak English well (repsonse "3").Total, there are more English speaker males than females, but this makes sense given that there are more men in the sample.




# Exam 1

### Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (fatal-police-shootings-data.csv) onto that folder, and save your Exam 1.Rmd file in the same folder.

c. Download the README.md file. This is the codebook. 

d. Load the data into an R data frame.
```{r}
dat <- (read.csv(file="data/fatal-police-shootings-data-copy.csv"))
```


### Problem 1 (10 points)

a. Describe the dataset. This is the source: https://github.com/washingtonpost/data-police-shootings . Write two sentences (max.) about this.

This dataset contains data for all of the fatal shootings by police in the US that have occurred since the beginning of 2015. Within the dataset is information about the name of the victim, date of shooting, whether they were armed, their age, gender, race, whether they were mentally ill, whether they were threatening, whether they were fleeing,presence of a body camera, and where it occurred.  

b. How many observations are there in the data frame?
```{r}
dim(dat)
```

There are 6,594 different observations of 17 different variables. 

c. Look at the names of the variables in the data frame. Describe what "body_camera", "flee", and "armed" represent, according to the codebook. Again, only write one sentence (max) per variable.
```{r}
names(dat)
```

"body_camera":Body camera refers to news reports that indicate whether the officer was wearing a body camera that may have recorded all or a portion of the incident. 
"flee": Flee refers to whether the victim was moving away from the officer, as indicated by news reports. 
"armed": Armed refers to whether or not the victim was in possession of a weapon (or an item that a police officer would perceive as harmful) during the time of the shooting. 

d. What are three weapons that you are surprised to find in the "armed" variable? Make a table of the values in "armed" to see the options.
```{r}
table(dat$armed)
```

I am surprised to see beer bottle, binoculars, and air conditioner as weapons in the "armed" variable because it is difficult to conceive how these could be perceived as harmful, especially when the police officer himself has a gun. 

### Problem 2 (10 points)

a. Describe the age distribution of the sample. Is this what you would expect to see?
```{r}
hist(dat$age , xlab = "Age of Shooting Victim", main = "Distribution of Age")
```

The age of the shooting victims is most concentrated between the ages of 20 and 40. This is generally what I would expect to see because 20 year olds are the most likely age category to commit to crime any way. Also, it is unlikely that a police officer would see someone over 60 or under 15 years old as a threat, which is why these age ranges are the least frequent in the dataset. 

b. To understand the center of the age distribution, would you use a mean or a median, and why? Find the one you picked.
```{r}
summary(dat$age)
```

To understand the center of the age distribution, you would use median because the data is skewed to the right. When data is skewed, median is a better measure of central tendency because it gives outliers or infrequent numbers less weight. The median of the age variable is 35.00. 

c. Describe the gender distribution of the sample. Do you find this surprising?
```{r}
table(dat$gender)
```

The gender distribution is very heavily weighted toward toward males. They outnumber the females by almost 20 times. This is not surprising because males are more likely to be perceived as threatening and to be criminal offenders. 


### Problem 3 (10 points)

a. How many police officers had a body camera, according to news reports? What proportion is this of all the incidents in the data? Are you surprised that it is so high or low?

```{r}
table(dat$body_camera)
```
According to the news report, 910 police officers had a body camera. This is less than 14% of all of the incidents in the dataset. I am surprised that it is so low because I was under the impression that body cams were a required for all police officers to wear. 

b. In  how many of the incidents was the victim fleeing? What proportion is this of the total number of incidents in the data? Is this what you would expect?
```{r}
table(dat$flee)
```

The victim was fleeing in 1,912 of the incidents. This represents apporoximately 1/6 of all the incidents recorded in the dataset. This is a little less than what I would expect because it is logical that a criminal would flee from a police officer in the hopes of not getting caught. 



### Problem 4 (10 points) -  Answer only one of these (a or b).

a. Describe the relationship between the variables "body camera" and "flee" using a stacked barplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the options for "flee", each bar contains information about whether the police officer had a body camera (vertically), and the height along the y-axis shows the frequency of that category).*

*Hint 2: Also, if you are unsure about the syntax for barplot, run ?barplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}
#$library(ggplot2) 
#ggplot(dat, aes(fill= dat$body_camera, y= frequency, x= dat$flee)) + #geom_bar(position="stack", stat="identity") 
#just so you know what I was thinking for my code



```

__Your answer here.__

b. Describe the relationship between age and race by using a boxplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the race categories and the height along the y-axis is age.* 

*Hint 2: Also, if you are unsure about the syntax for boxplot, run ?boxplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}
table(dat$race)
class(dat$race)
dat$age

class(dat$age)

boxplot(dat$age~dat$race)
```

The relationship between age and race show that white offenders seem to be older, while Black offenders are typically in a lower age category. It also appears that offenders older than 40  are more likely to be white or of unknown race. You can conclude from this relationship that older age is more frequent in white offenders and younger age is mor frequent in black offenders, with hispanics, native americans, and others closer in age generally to black offenders. 






### Extra credit (10 points)

a. What does this code tell us?
Answer: This code organizes the incidents by date and tells us the time difference among the incidents.

```{r, eval=FALSE}
mydates <- as.Date(dat$date)
head(mydates)
(mydates[length(mydates)] - mydates[1])
```

b. On Friday, a new report was published that was described as follows by The Guardian: "More than half of US police killings are mislabelled or not reported, study finds." Without reading this article now (due to limited time), why do you think police killings might be mislabelled or underreported?
Answer: Police killings might be mislabelled or underreported because it makes the gvoernment and police officers look bad. The police killings are attrocities and often envoke rioting and protests. The governemnt may see that it is in their best interest if these incidents are underreported to tamp down on backlash and avoid accountability. 

c. Regarding missing values in problem 4, do you see any? If so, do you think that's all that's missing from the data?
Answer: There are some missing values in the variable age. I think it is likely that there is also more missing from the data.

# Assignment 3

### Problem 1
Load the data.
```{r}
library(readr)
library(knitr)
dat.crime <- read_delim("/Users/hannah/Documents/Crim 250/data/crime_simple.txt", delim = "\t")
```
This is a dataset from a textbook by Brian S. Everitt about crime in the US in 1960. The data originate from the Uniform Crime Report of the FBI and other government sources. The data for 47 states of the USA are given. 

Here is the codebook:

R: Crime rate: # of offenses reported to police per million population

Age: The number of males of age 14-24 per 1000 population

S: Indicator variable for Southern states (0 = No, 1 = Yes)

Ed: Mean of years of schooling x 10 for persons of age 25 or older

Ex0: 1960 per capita expenditure on police by state and local government

Ex1: 1959 per capita expenditure on police by state and local government

LF: Labor force participation rate per 1000 civilian urban males age 14-24

M: The number of males per 1000 females

N: State population size in hundred thousands

NW: The number of non-whites per 1000 population

U1: Unemployment rate of urban males per 1000 of age 14-24

U2: Unemployment rate of urban males per 1000 of age 35-39

W: Median value of transferable goods and assets or family income in tens of $

X: The number of families per 1000 earning below 1/2 the median income


We are interested in checking whether the reported crime rate (# of offenses reported to police per million population) and the average education (mean number of years of schooling for persons of age 25 or older) are related. 


1. How many observations are there in the dataset? To what does each observation correspond?

```{r}
dim(dat.crime)
```

__There are 47 observations in this data set that correspond to one of each of the 47 states included in the data set across the 14 variables measured.__

### Problem 2
2. Draw a scatter plot of the two variables. Calculate the correlation between the two variables. Can you come up with an explanation for this relationship?

```{r, fig.width=6, fig.height=4}
plot(dat.crime$Ed, dat.crime$R,  main="Relationship between Average Education and Reported Crime Rate",
    xlab="Average Education", ylab="Crime Rate")
```
```{r}
cor(dat.crime$Ed, dat.crime$R)
```
__The variables are loosely positively correlated, but the correlation is not that strong, as illustrated by the scatter plot and the correlation coefficient value of 0.3228349 that is not close to 1. One explanation for this relationship is that states with larger urban centers, especially in terms of high-level job opportunities, tend to have people with higher levels of education working those jobs. These same urban places also tend to be characterized by higher crime rates. It is possible that the positive correlation between the average education and crime rate is a product of the large cities themselves.__

### Problem 3
3. Regress reported crime rate (y) on average education (x) and call this linear model `crime.lm` and write the summary of the regression by using this code, which makes it look a little nicer `{r, eval=FALSE} kable(summary(crime.lm)$coef, digits = 2)`.

```{r} 
# Remember to remove eval=FALSE above!
dat.crime$Ed = scale(dat.crime$Ed, center=TRUE, scale=FALSE) # scaling the data
crime.lm <- lm(formula = R ~ Ed, data = dat.crime) # running regression
summary(crime.lm) # calling the summary of the fitted model
```
### Problem 4
4. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.)

```{r}
plot(crime.lm, which=1)
```
Testing linearity assumption.

```{r}
plot(dat.crime$Ed, crime.lm$residuals, ylim=c(-15,15), main="Residuals vs. x", xlab="x, Average Education", ylab="Residuals")
abline(h = 0, lty="dashed")
``` 
Testing linearity and Independence assumption above. 

```{r}
plot(crime.lm, which=3)
```
Testing variance assumption/homoscedascisity above.
```{r}
plot(crime.lm, which=5)
```

```{r} 
plot(crime.lm, which=2)
```

Testing Normal Population Assumption above.

__Linearity Assumption:The data passes this assumption because the line in the Residuals vs. Fitted plot is mostly flat and the data distribution shows no pattern that would suggest a non-linear relationship. Independence Assumption:The data passes this assumption because the Residuals vs. X plot shows a pretty well spread out distribution with no noticeable, distinct pattern. Variance Assumption:The data passes this assumption because the line in the Scale-location plot is relatively straight, and there are about the same number of data points on either side of the line, which suggests the errors have constant variance. Normal Population Assumption: This assumption is not met because the data points in the top right of the normal Q-Q plot don't represent a normal distribution, and we know that these data points are not outliers based on their location within Cook's distance in the Residuals vs. leverage plot. __

### Problem 5
5. Is the relationship between reported crime and average education statistically significant? Report the estimated coefficient of the slope, the standard error, and the p-value. What does it mean for the relationship to be statistically significant?

__The relationship between reported crime and average education is statistically significant based on the following values. The estimated coefficient of the slope is 1.1161. The standard error is 0.4878. The p-value is 0.02688. When a relationship is statistically significant, it means that the two variables are likely related, meaning we can reject the null hypothesis.__

### Problem 6
6. How are reported crime and average education related? In other words, for every unit increase in average education, how does reported crime rate change (per million) per state?

__For every unit increase in average education, reported crime per million per state is predicted to increase by 1.1161 offenses. __

### Problem 7
7. Can you conclude that if individuals were to receive more education, then crime will be reported more often? Why or why not?

__You cannot conclude that if individuals were to receive more education, then crime will be reported more often because there is not evidence of a causal relationship here. Correlation does not equal causation. Just because the events are associated with one another does not mean that one causes the other. It is possible in this case that there is a third, unseen variable that causes both of these trends. Also, states are being measured here not individuals, so it would be difficult to make a prediction about individuals based on this model.__

# Exam 2
### Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (sim.data.csv) onto that folder, and save your Exam 2.Rmd file in the same folder.

c. Data description: This dataset provides (simulated) data about 200 police departments in one year. It contains information about the funding received by the department as well as incidents of police brutality. Suppose this dataset (sim.data.csv) was collected by researchers to answer this question: **"Does having more funding in a police department lead to fewer incidents of police brutality?"**
d. Codebook:
- funds: How much funding the police department received in that year in millions of dollars.
- po.brut: How many incidents of police brutality were reported by the department that year.
- po.dept.code: Police department code

### Problem 1: EDA (10 points) 

Describe the dataset and variables. Perform exploratory data analysis for the two variables of interest: funds and po.brut.

```{r}
dat<- read.csv (file ='/Users/hannah/Documents/Crim 250/Test Github/Hannah-R/data/sim.data copy.csv')
```

```{r}
plot(dat$funds, dat$po.brut, main = "Funding Versus Police Brutality", xlab = "Funding Milllions of Dollars", ylab = "Incidents of Reported Police Brutality")
```
__The data set contains 200 observations of 3 different variables.The variables are police department code, funds and police brutality. Police department code is a qualitative variable, as it just tells you what police department the information is about. Police brutality is a quantitative variable because it has units and can be evaluated for mean, median, and mode etc. Funds is also a quantitative variable for the same reason. I chose to do a scatter plot above because funding and number of reported police brutality are both quantitative variables. This scatter plot seems to suggest a strong negative correlation between funding and police brutality, meaning that as funding increases brutality seems to decrease; however, there is no way to know conclusively based solely on this scatter plot. __


### Problem 2: Linear regression (30 points)

a. Perform a simple linear regression to answer the question of interest. To do this, name your linear model "reg.output" and write the summary of the regression by using "summary(reg.output)". 

```{r}
# Remember to remove eval=FALSE!!
dat$funds = scale(dat$funds, center=TRUE, scale=FALSE) 
reg.output <- lm(formula = po.brut ~ funds, data = dat)
summary(reg.output)
```


b. Report the estimated coefficient, standard error, and p-value of the slope. Is the relationship between funds and incidents statistically significant? Explain.

__The estimated coefficient is -.367099. The standard error is 0.004496. The p-value of the slope is 2.2e-16. These numbers indicate that the relationship between funds and police brutality are statistically significant because the p-value is much less 0.05.__

c. Draw a scatterplot of po.brut (y-axis) and funds (x-axis). Right below your plot command, use abline to draw the fitted regression line, like this:
```{r, fig.width=4, fig.height=4, eval=FALSE}
# Remember to remove eval=FALSE!!
plot(dat$funds, dat$po.brut, main = "Funding Versus Police Brutality", xlab = "Funding Milllions of Dollars", ylab = "Incidents of Reported Police Brutality")
abline(reg.output, col = "red", lwd=2)
```
Does the line look like a good fit? Why or why not?

__This line looks like a fairly good fit because many of the data points fall on or near the line. There also are not any distinct and recognizable patterns of extreme variation from the line. The line follows the general pattern of the actual data, which is a good indicator that it is a good fit at this stage.__

d. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.) If not, what might you try to do to improve this (if you had more time)?

```{r}
#testing the linearity assumption (residuals vs. fitted)
plot(reg.output, which = 1)

```

```{r}
#testing the linearity and independence assumptions (residuals vs. x) 
plot(dat$funds, reg.output$residuals, ylim = c(-15,15), main = "Residuals vs. X", xlab = "X, Funds", ylab = "Residuals")
abline(h = 0, lty = "dashed")
```

```{r}
#Testing the equal variance assumption (scale location plot)
plot(reg.output, which=3)
```

```{r}
#Testing normal population assumption (residuals vs. leverage)
plot(reg.output, which=5)
```

```{r}
#Testing normal population assumption (normal qq)
plot(reg.output, which=2)
```
__The linearity assumption is not met because the line that fits the data is a curve, which suggests that the data may have a nonlinear relationship. The independence assumption does not appear to be met because there is a very distinct pattern in the data as much of it is concentrated in the curved middle section between -20 and 20. The equal variance assumption is also not met because the fit line is not very close to flat, which suggests that the errors have non-constant variance. The normal population assumption is not met because the normal Q-Q plot appears to have a left skew, meaning the residuals are not likely normally distributed.__

e. Answer the question of interest based on your analysis.

__With only the linear regression model, it is impossible to formulate a good answer to whether more funding in a police department will lead to fewer incidents of police brutality. Since none of the assumptions are satisfied, a linear model is not a good fit and will not make good predictions for this data. It may be possible to make better predictions if we do further analysis by transforming the data in some way. __

### Problem 3: Data ethics (10 points)

Describe the dataset. Considering our lecture on data ethics, what concerns do you have about the dataset? Once you perform your analysis to answer the question of interest using this dataset, what concerns might you have about the results?

__The data set has data for 200 police departments in one year. In order to answer the question of interest, more analysis has to be done because the linear regression model is insufficient to make a prediction in this case. In terms of data ethics, it is concerning that police departments have control over what gets reported. For instance, they could under report the incidences of police brutality in order to make themselves look better. This means that the data may not be truly representative of this situation. Models can often reinforce biases if the models are based on biased data. This could be an issue in this case because the police have control over how many instances of brutality get reported.The results could be reflecting lower levels of brutality than are actually present, which affects our interpretation of the relationship between funding and brutality. __

# Assignment 4

###Chapter 3: Data Visualization 
```{r}
#install.packages("tidyverse")
library(tidyverse)

#these two lines downlaoded and installed the tidyverse package that is necessary for the following lines of code to work. 
```

```{r}
mpg
#This line of code shows the mpg data frame in ggplot2.
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
#The first line says that we want to use the mpg data frame in ggplot.
#The second line creates the mapping plot below by adding a layer of points to the graph with car's engine size on the x-axis and car's fuel efficiency on the y-axis.

#ggplot(data = <DATA>) + 
# <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>)) THIS IS A TEMPLATE FOR MAKING OTHER GRAPHS IN GGPLOT 
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))
#The first line of code is the same as before: it sets up that we're going to make a graph in ggplot using the mpg data frame.
#The second line includes the layer of points and adds a third variable of class to plot by using color as an aesthetic. The car classes are differentiated by color. This adds another level to the graph, which is known as scaling.It also creates a legend to explain the mapping.  
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, size = class))
#This code does a very similar as above, but instead uses size of the points as a level to distinguish the third variable. It also creates a legend to explain the mapping. 
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))
#Very similar as the previous codes, but this maps class to the alpha aesthetic, which distinguishes the variable by transparency.This adds another layer to the plot. It also creates a legend to explain the mapping. 

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, shape = class))
#This code is similar again but maps class to the shape aesthetic, which distinguishes the class variable by point shape.This adds another layer to the plot.It also creates a legend to explain the mapping.  
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
#This adds an aesthetic to the plot manually. The color="blue" turns all the points blue. It just adds to the appearance of the graph, but it does not convey any additional information about the variable.
```

```{r}
#ggplot(data = mpg) 
#+ geom_point(mapping = aes(x = displ, y = hwy))
#This graphing code from before is not working because the + is at the beginning of the second line. It has to be the at the end of the first line in order to work. 
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
#This sets up the same plot we have been using, but the third row adds a level to the graph called facets. Facets are subplots that display subsets of the data separately. Now, differences in the class variable are set apart by subplots as opposed to the aesthetic levels we were adding previously.Also, nrow=2 creates 2 rows and 2 columns. 
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
#This does something similar as above, but there is an important difference. The code facet_grid contains 2 variables and facets the plot as a combination of the 2. Also, there is no setting of rows and columns here.  
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
#The first line says we're using ggplot and the mpg data frame. The second line establishes that we're going to use points as our geometrical object to represent our data. 

ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy))
#The second line in this code sets smooth as our geom for the plot, which represents data in a line. 
```
```{r}
ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
#The addition of the third line here adds an aesthetic that sets a line type distinction. This distinction separates the data by the third variable drv. Drv classifications are now clear in this geom. 
```

```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
              
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))
    
ggplot(data = mpg) +
  geom_smooth(
    mapping = aes(x = displ, y = hwy, color = drv),
    show.legend = FALSE
  )
#This code shows 3 different variations of the geom_smooth function. The first has all the data in one line and displ on the x-axis and hwy on the y-axis. The second plot adds the drv variable, which creates 3 different lines by category in that variable. The third plot adds the aesthetic of color to the drv variable to add further distinction. 
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
#This code integrates multiple goems of the same data into one plot. The second line creates the point geom, and the third line creates the smooth geom. Their location under the original ggplot code and the + signs allow the two geoms to occupy one plot together. 
```

```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()
#This code makes the same plot as above, but the variables are determined in the first line rather than the subsequent two. This makes it easier to change the x and y-axes if we want a different display because the variables are in one location of the code as opposed to two. 
```

```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth()
#This code creates a similar plot as above, but in the second line it adds color as an aesthetic to the point layer of the display.
```

```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)
#This code creates a very similar plot but the third line turns the geom_smooth plot into only a representation of the "subcompact" subset of the data. 
```

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut))
#The first line identifies diamonds as the data set we are using. The second line creates a bar plot with cut variable on the x-axis and count on the y-axis. 
```

```{r}
ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
#This code creates the same plot as above, because the geom_bar and stat_count code functions are interchangeable. 
```

```{r}
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)

ggplot(data = demo) +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")
#The final line of code changes the geom_bar default stat from count to identity. This maps the height of the bars to raw y variable values.
```

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))
#This code creates a bar chart of proportion as opposed to count. 
```

```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.min = min,
    fun.max = max,
    fun = median
  )
#This code creates a visual summary of the data with variables cut and depth. It summarizes the depth values for each unique cut value. 
```

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, colour = cut))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut))
#This code takes the original bar chat of cut and count of the diamond data set and adds color to the bars that differ based on x-value designation. The colour function in the second line of code makes colored borders of the bars. The fill function colors each bar fully. 
```

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))
#This code maps the fill aesthetic to another variable, clarity. Now the colors are stacked withing the cut bars and refer to levels of clarity. 
```

```{r}
ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5, position = "identity")
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")
#The first line of the code makes the the bar plot with the cut variable on the axis and the clarity variable as the color fill. The next line places each object precisely where it falls in the graph, which necessitates making the bars slightly transparent. This is accomplished by setting alpha to a small value. The following line creates a plot of the same variables, but uses colour instead of fill. The final line makes the bars completely transparent by making fill=NA    
```

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")
#This is another similar plot but the position is set to "fill," which makes all of the bars the same height. With same height bars, it becomes easier to compare the color difference across x-variable groups. 
```

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
#Instead of stacking the overlapping objects, setting the position to "dodge" displays them side-by-side, which also helps make comparison easier. 
```

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
#The first line established mpg as our data set. The second line creates a scatter plot with displ on the x-axis and hwy on the y-axis. Setting the position to "jitter" in the third line avoids the issue over plotting by adding a little randomness to each point.  
```

```{r}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot() +
  coord_flip()
#This code creates a boxplot with class on the x-axis and hwy on the y-axis. It then creates this plot again and uses coord_flip to flip the x and y axes. 
```

```{r}
nz <- map_data("nz")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black")

ggplot(nz, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "black") +
  coord_quickmap()
#This code creates a map of New Zealand. The fourth and fifth lines then create the same map and add to it in line 6. It uses coord_quickmap to correctly set the aspect ratio, so it does not appear distorted.
```

```{r}
bar <- ggplot(data = diamonds) + 
  geom_bar(
    mapping = aes(x = cut, fill = cut), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_flip()
bar + coord_polar()
#The first part of this code creates a colored bar plot of the diamonds data set with cut on the x-axis and the fill criteria. It then flips the x and y-axes and uses polar coordinates (in the last two lines of the code), which allows comparisons to be drawn betweem the bar chart and the Coxcomb chart. 
```

```{r}
#ggplot(data = <DATA>) + 
#  <GEOM_FUNCTION>(
#     mapping = aes(<MAPPINGS>),
#     stat = <STAT>, 
#     position = <POSITION>
#  ) +
#  <COORDINATE_FUNCTION> +
#  <FACET_FUNCTION>
#This is a template that contains all 7 of the ggplot2 parameters covered above. 
```

###Chapter 28: Graphics for Communication 
```{r}
#install.packages("ggrepel")
#install.packages("viridis")
library(ggrepel)
library(viridis)
#These 4 lines of code install and download the ggrepel and viridis codes to be used below. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(title = "Fuel efficiency generally decreases with engine size")
#This code creates a point geom plot and a line geom plot for the variables displ and hwy. The point plot also includes colors based on the variable class. The labs function allows the addition of a plot title. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight",
    caption = "Data from fueleconomy.gov"
  )
#This code creates the same plot as above but adds both a subtitle and a caption in addition to the plot title. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  labs(
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    colour = "Car type"
  )
#This creates the same plot again but uses labs to to assign axis titles and the color legend. 
```

```{r}
df <- tibble(
  x = runif(10),
  y = runif(10)
)
ggplot(df, aes(x, y)) +
  geom_point() +
  labs(
    x = quote(sum(x[i] ^ 2, i == 1, n)),
    y = quote(alpha + beta + frac(delta, theta))
  )
#This code allows you to put a mathematical equation as the axis label as opposed to string of text. 
```

```{r}
best_in_class <- mpg %>%
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1)

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_text(aes(label = model), data = best_in_class)
#This code creates a geom point plot and adds annotations to the plot. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5)
#This code creates a similar plot, but the addition of the geom_label code puts a rectangle behind the text, which makes it easier to read. Also, nudge_y moves the labels slightly above the points they refer to. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_point(size = 3, shape = 1, data = best_in_class) +
  ggrepel::geom_label_repel(aes(label = model), data = best_in_class)
#This code adds a second layer of labels and large hollow points that makes the annotations even more clear. 
```

```{r}
class_avg <- mpg %>%
  group_by(class) %>%
  summarise(
    displ = median(displ),
    hwy = median(hwy)
  )
#> `summarise()` ungrouping output (override with `.groups` argument)

ggplot(mpg, aes(displ, hwy, colour = class)) +
  ggrepel::geom_label_repel(aes(label = class),
    data = class_avg,
    size = 6,
    label.size = 0,
    segment.color = NA
  ) +
  geom_point() +
  theme(legend.position = "none")
#This code replaces the color legend with labels directly on the plot. The last line removes the traditional legend. 
```

```{r}
label <- mpg %>%
  summarise(
    displ = max(displ),
    hwy = max(hwy),
    label = "Increasing engine size is \nrelated to decreasing fuel economy."
  )

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
#This code creates geom point plot with one single label on the plot. It also uses a specific data frame by using the summarise function. 
```

```{r}
label <- tibble(
  displ = Inf,
  hwy = Inf,
  label = "Increasing engine size is \nrelated to decreasing fuel economy."
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")
#This code creates a very similar plot as above but moves the label to the border of the plot using the vjust and hjust codes. 
```

```{r}
"Increasing engine size is related to decreasing fuel economy." %>%
  stringr::str_wrap(width = 40) %>%
  writeLines()
#> Increasing engine size is related to
#> decreasing fuel economy.
#This code creates the same plot but uses the more succinct command stringr::str_wrap, which automatically adds line breaks by giving a space to designate characters per line. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))
#This code creates a geom point plot for the displ and hwy variables of the mpg data set. The colour function applies the class variable in the form of coloring the dots differently. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  scale_x_continuous() +
  scale_y_continuous() +
  scale_colour_discrete()
#This code creates the same plot as above but adds scale parameters. 
```


```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_y_continuous(breaks = seq(15, 40, by = 5))
#This code creates a point geom plot for the displ and hwy variable of the mpg data set. It then sets the scale axes and uses the breaks function to place the ticks at certain values. This makes the x-axis range from 15 to 40 with ticks at every 5th value. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_x_continuous(labels = NULL) +
  scale_y_continuous(labels = NULL)
#This code create a similar plot with the same data and variables but removes the values from the axes by setting their scales to null. 
```

```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y")
#This plot shows what year each US president started and ended their terms. This means that each president on the y-axis will correspond to more than one of the years on the x-axis. Thus uses the breaks function to show exactly where the observations occur.  
```

```{r}
base <- ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))

base + theme(legend.position = "left")
base + theme(legend.position = "top")
base + theme(legend.position = "bottom")
base + theme(legend.position = "right") # the default
#This code creates 4 different versions of the same geom point plot using the displ and hwy variables of the mpg data set. It also uses color to distinguish the class variable. Each of the different versions of this plot uses the legend.position function to control where on the plot the legend is drawn. This is a theme() setting. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  theme(legend.position = "bottom") +
  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4)))
#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'
#This code plots the same variables as before, using both a point geom and a smooth geom with color representing the class variable. The code also puts the legend at the bottom. Nrow sets the number of rows for the legend (in this case 1) and override.aes makes the points of the legend bigger. 
```

```{r}
ggplot(diamonds, aes(carat, price)) +
  geom_bin2d()

ggplot(diamonds, aes(log10(carat), log10(price))) +
  geom_bin2d()
#This code makes two plots of carat and price from the diamonds data set. The first uses a regular scale, while the second uses a log10 scale to make the relationship between the variable counts more obvious. 
```

```{r}
ggplot(diamonds, aes(carat, price)) +
  geom_bin2d() + 
  scale_x_log10() + 
  scale_y_log10()
#This code creates a similar plot, but the scale of the axes is changed through a scale function and not an aesthetic function, which ensures that the axes are labelled on the orginal scale. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv))

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv)) +
  scale_colour_brewer(palette = "Set1")
#This code creates 2 versions of the same scatter plot but the second sets a specific color palette, while the first just employs the default. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv, shape = drv)) +
  scale_colour_brewer(palette = "Set1")
#This code creates a scatter plot of the same data, but it adds shape as an aesthetic to further distinguish which group of the drv variable the points belong to. 
```

```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id, colour = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue"))
#This code creates the same presidential plot from before but it adds party as a third variable to categorize the data. The scale_colour_manual allows for the colors to be included and put into the legend. 
```

```{r}
df <- tibble(
  x = rnorm(10000),
  y = rnorm(10000)
)
ggplot(df, aes(x, y)) +
  geom_hex() +
  coord_fixed()

ggplot(df, aes(x, y)) +
  geom_hex() +
  viridis::scale_fill_viridis() +
  coord_fixed()
#This code creates 2 versions of the same hex plot. The first one  allows the colors to be default. The second plot, however, uses the viridis package to apply a specific color scheme. This is accomplished by viridis::scale_fill_viridis/
```

```{r}
ggplot(mpg, mapping = aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth() +
  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30))

mpg %>%
  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) %>%
  ggplot(aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth()
#This code creates 2 plots that contain both points and smooth lines of the displ, hwy, and class variable of the mpg data set. The coord-cartesian line of code zooms in on a specific region of the plot.  
```

```{r}
suv <- mpg %>% filter(class == "suv")
compact <- mpg %>% filter(class == "compact")

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point()

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point()
#This code creates 2 plots with displ, hwy, and drv. One of the them plots the suv class of cars and one of them plots the compact class of cars. This makes them difficult to compare because the scales have different ranges. 
```

```{r}
x_scale <- scale_x_continuous(limits = range(mpg$displ))
y_scale <- scale_y_continuous(limits = range(mpg$hwy))
col_scale <- scale_colour_discrete(limits = unique(mpg$drv))

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale
#This code improves upon the previous plots by using the limits function to share the scales across both plots. These plots contain the same information as above, but the standardization of the scale makes them easier to compare. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  theme_bw()
#This code creates a point and smooth line plot of the displ, hwy, and class variables of the mpg data set. It uses theme_bw to create a white background with grid lines. 
```

```{r}
ggplot(mpg, aes(displ, hwy)) + geom_point()
#This code creates a scatter plot of displ and hwy with no added color. 
```

```{r}
ggsave("my-plot.pdf")
#> Saving 7 x 4.33 in image
#This code saves an image of the above plot. 
```

# Final Project


## Marijuana Possession Charges
### An exploratory look at the relationship between race and sentencing outcomes

#### INTRODUCTION
Following Nixon's call for the War on Drugs in June 1971, there was a push for mandatory sentencing in drug-related crimes, effectively expanding the role of the federal government in drug-related arrests and sentencing (Drug Policy Alliance, 2021). Recently, there has been an uptick of discussion in the political world and in pop culture of the United States on the War on Drugs, namely regarding the adjustment of the severity of sentencing and its disproportionate effect on minority communities. These racial disparities are particularly discussed in regards to marijuana-related charges, as Black Americans have been found to be four times more likely to be arrested for marijuana charges than White Americans and six times more likely to be incarcerated for drug-related charges (Rahamatulla, 2017). Many agree that a solution to the disproportionate effects of drug-related charges needs to be created, and through the work discussed in this paper, we hope to focus our attention on sentencing outcomes and race for marijuana charges to further understand where disparities lie. Due to the increasing prevalence and history of drug-related sentencing, we wanted to examine the relationships in drug-related sentencing between race and sentencing likelihood, especially focusing on marijuana. We expect, from past studies examined in and outside of this course, that the rate of prison sentencing for Black people would be higher than the rate of prison sentencing of white people. These motivations lead to the research question: Are Black people sentenced federally to prison for marijuana at a higher rate than white people?

#### DESCRIPTION OF THE DATA USED
The data set we utilized for this review is a record of federal criminal sentences as provided by the US Sentencing Committee (United States Sentencing Commission, 2007). This data set includes information on federal cases sentenced under the guidelines of the Sentencing Reform Act of 1984. Because of the research question prompting this paper, we chose to look at only cases where the defendant was charged with possession of marijuana. We used the Drug Type 1 variable in the dataset to create a subset with these cases, as this variable indicated that marijuana was the highest penalty incurring drug the defendant was found with. Because we wanted to look at the relationship between race and being sentenced to prison, our independent variable was the defendants race (either Black or white cases) and our dependent variable was the type of sentence (either prison or no prison). We extrapolated specifically Black and white instantiations in the race variable so as to control for the number of independent variables being observed. We also decided to control for the defendants age, the defendants gender, and whether or not the defendant has a criminal record. We thought that these three variables would be the most influential in determining sentence type, both as legal (criminal record) and extralegal (age and gender) variables.

```{r, echo=FALSE}
#Reading in our dataset from the federal sentencing 
library(readr)
library(knitr)
originaldata <- read.table("/Users/hannah/Documents/Crim 250/data/dat.tsv", sep = '\t', header = TRUE)
```

```{r, echo=FALSE}
#Creating a subset of the dataset so that it is easier to work with

dat <- subset(originaldata, select = c(USSCIDN, AGE, MONRACE, MONSEX, CRIMHIST, DRUGTYP1, SENTIMP))

dat1 <-  dat[ which(dat$DRUGTYP1==4) , ]

#The variables we are working with are
#1. USSCIDN = Unique Case ID Number
#2. AGE = Defendant Age
#3. MONRACE = Defendant Race
#4. MONSEX = Defendant Gender
#5. CRIMHIST = If the Defendant has a Criminal History
#6. DRUGTYP1 = Drug Type (Looking specifically at marijuana)
#7. SENTIMP = Type of Sentence 
```

```{r, echo=FALSE}
#We created white, black, prison, and non prison variables to complete the analyses. 

dat2 <-  dat1[ which( dat1$MONRACE==1 | dat1$MONRACE==2 ) , ] # only keep white and black cases
dat2$race <- dat2$MONRACE

dat2$white <- NA
dat2$white[dat2$race == "1"] <- 1
dat2$white[dat2$race == "2"] <- 0

dat2$black <- NA
dat2$black[dat2$race == "1"] <- 0
dat2$black[dat2$race == "2"] <- 1

dat2$prison <- ifelse(dat2$SENTIMP==1 | dat2$SENTIMP==2, 1, 0) # 0: not prison, 1: prison
```

#### EXPLORATORY DATA ANALYSIS
```{r, echo=FALSE, results=FALSE}
#We made a bar plot of being sentenced and race

library(ggplot2)

sum(dat2$black) #763
sum(dat2$white) #5365

table(dat2$black,dat2$prison)

#white and no prison: 299
#white and prison : 5061
#black and no prison: 80
#black and prison: 683

#percentage of the time that someone is sentenced to prison when they are black
683/763 # 0.89515

#percentage of the time that someone is not sentenced to prison when they are black
80/763 # 0.10485

#percentage of the time that someone is sentenced to prison when they are white
5061/5365 # 0.94334

#percentage of the time that someone is not sentenced to prison when they are black
299/5365 #0.05573

labs <- c("Black", "Black", "White", "White")

prison.graph <- data.frame(avg.prop=c(0.90, 0.10, 0.94, 0.06),
                 Legend=c("Prison", "No Prison"), labs)

prison.bar <- ggplot(prison.graph, aes(x = Legend, y = avg.prop, fill = Legend)) + 
  geom_bar(stat="identity", position = "dodge") +
  geom_text(aes(label=avg.prop), vjust=-0.3, size=4) +
  ylab("Average Proportion of Being Sentenced to Prison") +
  scale_fill_manual(values = c("indianred1", "darkturquoise")) +
  facet_wrap(vars(labs),
             ncol = 2, 
             nrow = 1) +
  xlab("Sentence Outcome") +
  ggtitle("Average Proportion of Being Sentenced to Prison Depending on Race") +  
  theme(plot.title = element_text(hjust = 0.3)) +
  theme_bw() +
  scale_y_continuous(limits = c(0, 1)) + 
  scale_x_discrete(labels=c("No Prison", "Prison", "No Prison", "Prison"))
prison.bar

ggsave(filename = "prison.bar.png", 
       plot = prison.bar,
       width = 12, 
       height = 7)
```
To most successfully represent the variables being examined in this paper, we chose to create a bar plot representing the proportion of people sentenced and not sentenced to prison for both races included in the dataset. 

#### REPRESENTATIVE MODEL AND DIAGNOSTIC INFORMATION
For the sake of our data set, we used a logistic regression. A logistic regression model is very similar to a linear regression; however, it is used to evaluate relationships between binary variables. Because our variables - race and sentencing outcome - both have only two options for the purpose of our study, we can examine this relationship between the binaries. Rather than a standard line, the graph of fit is a logit. The application of this model is to categorical variables, and the dependent variable matches the variable type of the independent.


```{r, echo=FALSE, results=FALSE}
#We first ran a baseline logistic regression to look at the relationship between race and type of sentence 

reg.baseline <- glm(prison ~ black, family=binomial(link='logit'), data=dat2)
summary(reg.baseline)
```

#### LOGISTIC REGRESSION ASSUMPTIONS
```{r, echo=FALSE}
#We next checked the assumptions of logist regression:

diag.fun.glm = function(glm.out){
  n <- dim(glm.out$model)[1]
  par(mfrow=c(2,3))
  
  # qq plot
  #plot(glm.out, 2)
  qqnorm(rstandard(glm.out, type="deviance"), main="QQ plot of stand. dev. residuals")
  qqline(rstandard(glm.out, type="deviance"))
  
  # std. deviance vs. fitted values with horizontal lines at -2, 2
  plot(predict.glm(glm.out, type="response"), rstandard(glm.out, type="deviance"), ylab="Stand. deviance resid.", xlab="Fitted values", main="Stand. dev. resid. vs. fitted values"); abline(h=c(0,-2,2), lty=c(1,2,2))
  
  # studentized residuals
  alpha <- 0.05
  hi <- -qnorm(alpha/(2*n),0,1)
  lo <- qnorm(alpha/(2*n),0,1)
  plot(predict.glm(glm.out), rstudent(glm.out, type="deviance"), ylab="Stud. deviance resid.", xlab="Fitted values", main="Stud. dev. resid. vs. fitted values", ylim=c(lo-2,hi+2)); abline(h=c(0, hi, lo), lty=c(1,2,2))  
  
  # cook's distance plot
  cooks <- cooks.distance(glm.out)
  plot(cooks, type="h", main="Cook's distance", ylab="Cook's distance", xlab="Index")
  
  # leverage plot
  leverage <- hatvalues(glm.out) 
  plot(leverage, type="h", main="Leverage", ylab="Leverage", xlab="Index")
  
  # cook's distance vs. leverage plot
  plot(glm.out, 5)
}

#calling all of the plots
diag.fun.glm(reg.baseline)
```

For the sake of our data set, we used a logistic regression. A logistic regression model is very similar to a linear regression; however, it is used to evaluate relationships between binary variables. Because our variables - race and sentencing outcome - both have only two options for the purpose of our study, we can examine this relationship between the binaries. Rather than a standard line, the graph of fit is a logit. The application of this model is to categorical variables, and the dependent variable matches the variable type of the independent.

The Residual vs. Fitted Plots in this figure look to see if there are any curvilinear trends in the plots that were originally missed. Because logistic regression is already curvilinear - as demonstrated by the logit that was previously displayed - these plots do not tell us any definitive information on the validity of this regression. 

The QQ plot in the figure determines if the residuals are normally distributed. This plot is not indicative of anything definitive either because residuals do not have to be normally distributed in a logistic regression. 

The Residuals vs. Leverage plots help identify outliers, but this plot too is not particularly useful because the results are not definitive either. 
From the models demonstrated in the assumptions, we do not gain information that definitively determines the strength of the logistic regression model for our data set, but the assumption display is crucial to data analysis so as to examine any particularly significant data that strays from the norm. 

#### Regression Model
```{r, echo=FALSE}
#We created binary control variables

#creating age group variables
dat2$age1629 <- 0
dat2$age1629[dat2$AGE >= 16 & dat2$AGE <= 29] <- 1

dat2$age3049 <- 0
dat2$age3049[dat2$AGE >= 30 & dat2$AGE <= 49] <- 1

dat2$age5069 <- 0
dat2$age5069[dat2$AGE >= 50 & dat2$AGE <= 69] <- 1

dat2$age7097 <- 0
dat2$age7097[dat2$AGE >= 70 & dat2$AGE <= 97] <- 1

#creating gender variables
dat2$male <- NA
dat2$male[dat2$MONSEX == "0"] <- 1
dat2$male[dat2$MONSEX == "1"] <- 0

dat2$female <- NA
dat2$female[dat2$MONSEX == "0"] <- 0
dat2$female[dat2$MONSEX == "1"] <- 1

#creating criminal history variables
dat2$crimyes <- NA
dat2$crimyes[dat2$CRIMHIST == "0"] <- 0
dat2$crimyes[dat2$CRIMHIST == "1"] <- 1

dat2$crimno <- NA
dat2$crimno[dat2$CRIMHIST == "0"] <- 1
dat2$crimno[dat2$CRIMHIST == "1"] <- 0
```

```{r, echo=FALSE, results=FALSE}
#We then ran the same regression with our control variables

reg.controls <- glm(prison ~ black + age7097 + age5069  + age3049 + female + crimno, family=binomial(link='logit'), data=dat2)
summary(reg.controls)
```

```{r, echo=FALSE}
#Creating a table of the regression results
#install.packages("stargazer")
library(stargazer)
stargazer(reg.baseline, reg.controls, omit.stat = c("f","adj.rsq","ser","ll","aic","bic"), title="Logistic Regression Outputs", model.names = FALSE, column.labels = c("Baseline","Controls"), covariate.labels = c("Black", "Ages 70-97","Ages 50-69","Ages 30-49", "Female", "No Criminal History" , "Constant"), type="text", out="/Users/hannah/Documents/Crim 250/data/RegOutputs.txt")
```

Looking at the baseline regression, we can see that Black people in the dataset were less likely to be sentenced to prison at a significant rate. Similarly, looking at the controlled regression, we can see that Black people were more likely to not be sentenced when compared to white people in the dataset. We split the age variable into three groups (of 16-29, 30-49, 50-69, and 70-97), leaving the 16-29 group out of the regression as our control because we thought that this group would be most likely to be in possession of marijuana. To our surprise, all three age groups were more likely to be sentenced to prison for marijuana charges than people that were 16-29. This may be due to the fact that there could have been more people in these other groups, or there could have been a large number of minors in the 16-29 group that were not sentenced to prison. We split the gender variable into male or female, leaving out the male group since we thought that this group would be more likely to be sentenced to prison. Looking at the model, we can see that this is true, and females were significantly less likely to be sentenced to prison than males. Finally, we split the criminal history variable into yes or no groups, leaving out the yes group since we thought that this group would be more likely to be sentenced to prison. To our surprise, having no criminal history made the defendant more likely to be sentenced to prison, but by an insignificant amount.

#### CAUSAL ANALYSIS
Because we used the Drug Type 1 variable, we assumed that the defendant was either only found with marijuana or the other drug(s) did not incur any penalty. This did not take into account whether or not the defendant was charged with something incurring a felony charge, such as possession of drug paraphernalia or intent to sell. In an ideal world, our data would have been clear about what the defendant was actually charged with on all fronts, rather than just what drugs they were in possession of at the time of their arrest. We also only looked at about 6,000 observations, since we dropped everyone who was not Black or white. However, there were more white people in the dataset and more white people in prison in the dataset, so the groups may not have been proportional. 

##### DAG:
    
    BEING ARRESTED 
    ^           |
    |           v
    RACE -------> BEING SENTENCED TO PRISON

Additionally, the DAG portrays the situation where the defendants race determines whether or not they will be arrested, and therefore the arrest determines whether they will be sentenced. Not everyone who is caught with marijuana is arrested, and not everyone who is arrested for marijuana charges is convicted. Similarly, the biases of the police and prosecutors could be at play, influencing both the arrest and sentence outcome. Because of these confounders, we would not be able to conclude that there was a causal analysis.

##### LIMITATIONS AND FUTURE DIRECTIONS

While setting out to examine sentencing outcomes and their relationship to race for marijuana-related charges, we observed a series of confounds: 

1. Sentencing charges are not equated to arrests. It is worth looking in the future into the relationship between arrests and race for marijuana-related charges.
2. Our analysis did not include sentencing duration. Sentencing severity is a large component of the discussion for racial disparities in sentencing, not just the binary of whether someone was sentenced or not.
3. Though we controlled for different factors such as past arrests because they are crucial in determining sentencing outcomes they should be analyzed further in conjunction with the information we observed for future studies. 

This dataset is also a really interesting example of how a failure through data analysis can lead to false projections and misleading statistics. This is also a useful showcase of how easily data can also be manipulated depending on the neglect of specific outliers or parameters for the research.  As previously stated, this data set is very limited. We dont feel it can answer our research question in an accurate way, but it still provides a good lesson on the sensitivity of datasets with outlying variables and unobserved confounders.  

For future research, it could be extremely valuable to look at the current different sentencing rates across states with different laws regarding the legality and decriminalization of marijuana. All marijuana usage (whether medical or recreational) is a federal crime, so theoretically everyone in the dataset should have been arrested regardless of race. However, this is not the case, and it is important to look at how there are disparities between federal and state sentences. Additionally, state police and prosecutors have much more discretion in deciding who to arrest, and what crimes to charge. We also were only able to look at sentencing as a binary factor without the important information of arrests records in general or whether or not the defendant has been arrested for marijuana in the past. In the future, we want to do a more well-rounded in-depth data analysis including everyone who was arrested (regardless of their conviction status), as well as the states current laws regarding marijuana.  

#### References
Commission, U. S. S. (2014, June 25). Monitoring of federal criminal sentences, [united states], 2007.               Monitoring of Federal Criminal Sentences, [United States], 2007. Retrieved December 12, 2021, from https://www.icpsr.umich.edu/web/NACJD/studies/22623. 
Cusick Director, J., Cusick, J., Director, Director, C. M. A., Montecinos, C., Director, A., Director, S. H.  A., Hananel, S., Oduyeru Manager, L., Oduyeru, L., Manager, Gordon	Director, P., Gordon, P., Director, J. P. D., Parshall, J., Director, D., Pearl, B., Perez, M., Chung, E.,  Simpson,    E. (2021, October 28). Ending the war on drugs: By the numbers. Center for American Progress. Retrieved December 12, 2021, from https://www.americanprogress.org/article/ending-war-drugs-numbers/. 
Rahamatulla, A. (2017, March 23). The War on Drugs has failed. what's next? Ford Foundation. Retrieved December 12, 2021, from https://www.fordfoundation.org/just-matters/just-matters/posts/the-war-on-drugs-has-failed-what-s-next/. 
We Are The Drug Policy Alliance. (n.d.). A history of the Drug War. Drug Policy Alliance. Retrieved December 12, 2021, from https://drugpolicy.org/issues/brief-history-drug-war.